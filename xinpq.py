import requests 
from bs4 import BeautifulSoup 
from urllib.parse  import urlparse, urlunparse 
import re 
import time 
 
# ä»£ç†ç½‘å…³è·¯å¾„å¸¸é‡ï¼ˆæ ¸å¿ƒä¿®å¤ç‚¹ï¼‰
PROXY_PATH = "/wztz/https/www.yszzq.com" 
 
def build_proxy_url(original_url):
    """é‡æ„URLæ„å»ºé€»è¾‘ï¼ˆå…³é”®ä¿®å¤å‡½æ•°ï¼‰"""
    parsed = urlparse(original_url)
    if parsed.netloc  == "www.yszzq.com": 
        # æ„é€ å®Œæ•´ä»£ç†è·¯å¾„ï¼šåŸå§‹è·¯å¾„æ’å…¥åˆ°ä»£ç†ç½‘å…³è·¯å¾„å 
        new_path = f"{PROXY_PATH}{parsed.path}" 
        return urlunparse((
            parsed.scheme, 
            "wztz.wokaotianshi.eu.org", 
            new_path,
            parsed.params, 
            parsed.query, 
            parsed.fragment  
        ))
    return original_url 
 
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'X-Forwarded-Proto': 'https'  # æ–°å¢åè®®å¤´ 
}
 
urls = [
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk/",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_1",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_2",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_3",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_4",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_5",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_6",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_7",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_8",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_9",
    "https://wztz.wokaotianshi.eu.org/wztz/https/www.yszzq.com/tags/xmlcjjk_10"
]
 
all_results = []
 
for index, url in enumerate(urls):
    try:
        # æ™ºèƒ½è¯·æ±‚é—´éš”ï¼ˆé¿å…è§¦å‘åçˆ¬ï¼‰
        if index > 0:
            time.sleep(1.5) 
            
        response = requests.get(url,  headers=headers, timeout=15)
        response.raise_for_status() 
        
        soup = BeautifulSoup(response.text,  'lxml')
        pattern = re.compile(r' æ¥å£|åœ°å€|API|èµ„æº|èµ„æºåº“|èµ„æºæ¥å£|èµ„æºç½‘|json[\u4e00-\u9fa5]*', re.UNICODE)
        
        for element in soup.find_all(string=pattern): 
            parent = element.find_parent('a') 
            if not parent or 'href' not in parent.attrs: 
                continue 
                
            raw_href = parent['href']
            title = element.strip() 
            
            # è·¯å¾„å¤„ç†æµç¨‹ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
            if raw_href.startswith(('http://',  'https://')):
                final_url = build_proxy_url(raw_href)
            else:
                # ç›¸å¯¹è·¯å¾„å¤„ç† 
                if not raw_href.startswith('/'): 
                    raw_href = '/' + raw_href.lstrip('./') 
                final_url = f"https://wztz.wokaotianshi.eu.org{PROXY_PATH}{raw_href}" 
 
            # éªŒè¯ç¤ºä¾‹æ ¼å¼ 
            if "/ziyuan/api/" in final_url:
                print(f"Debug - Generated URL: {final_url}")
                
            if ("é‡‡é›†æ¥å£" in title or "èµ„æºåº“" in title or "èµ„æºæ¥å£" in title or "é‡‡é›†APIæ¥å£" in title ) \
            and "XML" not in title:
                all_results.append(f"{title},{final_url}") 
                print(f"âœ… Valid: {title[:15]}... -> {final_url[:50]}...")
 
    except Exception as e:
        print(f"âŒ Error on {url}: {type(e).__name__} - {str(e)[:50]}")
 
# æ–‡ä»¶å†™å…¥ 
with open('pq.txt',  'w', encoding='utf-8') as f:
    f.write('\n'.join(all_results)) 
print(f"ğŸ¯ ç»“æœå·²ä¿å­˜ï¼šå…±{len(all_results)}æ¡æœ‰æ•ˆè®°å½•")
